{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficult Exam questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam 2019/01/07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e. Clustering, 2 points\n",
    "What are the relative strengths and weaknesses of k-means clustering compared\n",
    "to agglomerative clustering? Indicate clearly for which approach a property is\n",
    "considered a relative strength or weakness and for what reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:  \n",
    "Relative strengths of agglomerative clustering include:\n",
    "* the output is more informative, corresponding to varing numbers of non-overlapping clusters\n",
    "* the output is derived deterministically ( no need for repeated runs)\n",
    "* the number of clusters does not need to be specified\n",
    "\n",
    "Relative strengths of k-means clustering include:\n",
    "* typically faster execution (heuristics)\n",
    "* will produce the desired number of clusters (if known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1., -1.,  0.],\n",
       "       [ 0., -1.,  0., -1.,  1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2a\n",
    "def random_projection(word_lists, rand_proj, dim):\n",
    "    res = []\n",
    "    for paragraph in word_lists:\n",
    "        row = np.zeros(dim)\n",
    "        for word in paragraph:\n",
    "            row += rand_proj[word]\n",
    "        res.append(row.tolist())\n",
    "    return np.array(res)\n",
    "\n",
    "word_lists = [[\"Python\"],[\"Python\",\"Julia\"]]\n",
    "rand_proj = {\"Python\":[0.0,0.0,1.0,-1.0,0.0],\"Julia\":[0.0,-1.0,-1.0,0.0,1.0]}\n",
    "dim = 5\n",
    "random_projection(word_lists, rand_proj, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b\n",
    "def decision_forest(dataframe, min_leaf, no_of_trees, no_of_features):\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    forest = np.empty(no_of_trees, dtype=\"object\")\n",
    "    features = list(df.columns)\n",
    "    if \"CLASS\" in features:\n",
    "        target = \"CLASS\"\n",
    "    elif \"REGRESSION\" in features:\n",
    "        target = \"REGRESSION\"\n",
    "    \n",
    "    features.remove(target)\n",
    "    orig_index = df.index\n",
    "    no_instances = df.shape[0]\n",
    "    for i in range(no_of_trees):\n",
    "        bootstrap_index = np.random.choice(orig_index, no_instances, replace=True)\n",
    "        feature_sample = np.random.choice(features, no_of_features, replace=False)\n",
    "        mod_df = df.loc[bootstrap_index,np.append(feature_sample, target)]\n",
    "        forest[i] = decision_tree(mod_df, min_leaf)\n",
    "    return forest\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam 2019/01/07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c\n",
    "Assume that we have generated a regression model for predicting the outdoor\n",
    "temperature using a large training set. However, we then ﬁnd out that the\n",
    "mean-squared error of the model is signiﬁcantly higher than of a default model,\n",
    "which just predicts the average outdoor temperature in the training set. Could\n",
    "our model still be more useful for prediction tasks than the default model?\n",
    "Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "If we would be interested in finding out which dats are expected to be the warmest, e.g., when planning some outdoor activities, then we are more interested in the correlation between the predicted and actual temperatures, rather than being interested primarily in the absolute temperatures. The trained model with a poorer MSE may hence be more useful for this purpose than the default model, it the correlation coefficient of the former is positive(while this is not the case for the default model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d\n",
    "Assume that we attempt to normalize numerical features using min-max nor-\n",
    "malization prior to learning a decision tree. What eﬀect will this preprocessing\n",
    "step have on the outcome of the decision tree learning algorithm? Consider\n",
    "both the case when numeric features are discretized before tree generation and\n",
    "during tree generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "If numeric features are discretized prior to tree generation using binning, then the relative frequencies in the bins are not affected by min-max normalization. Hence, the resulting tree will not be affected. Similarly, as normalization does not affect the relative order of the values, then the binary partitioning that can be obtained from the normalized and non-normalized feature, respectively, are the same, and again the resulting tree will not be affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e\n",
    "Is an association rule A →C always preferable to an association rule B →C, if\n",
    "the conﬁdence of the former is higher than the latter? Explain your reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "support(X->Y)=集合X与集合Y中的项在一条记录中同时出现的次数/数据的个数  \n",
    "confidence(X->Y)=集合X与集合Y中的项在一条记录中同时出现的次数/集合X出现的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "A rule with a high confidence may have a very low support, and hence may not be very accurate when applies to independent test instances (not included in the database from which the rules were generated). For example, a rule with a confidence of 100% may have a support of only one instance, and hence the conclusion may only hold in 50% of the test cases, while a rule with slightly lower confidence, but much higher support, can be expected to be more correct on independent test instances. Hence, confidence alone is not necessarily a meaningful evaluation criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a\n",
    "def select_fearures(df, no_features):\n",
    "    class_labels = df[\"CLASS\"]\n",
    "    res = [(evaluate(df[feature], class_labels),feature)\n",
    "              for feature in df.columns if feature != \"CLASS\"]\n",
    "    res.sort(reverse=True)\n",
    "    selected_features = [r[1] for r in result[:no_features]]\n",
    "    return df[[\"CLASS\"]+selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>M_0</th>\n",
       "      <th>res_0</th>\n",
       "      <th>x&gt;3</th>\n",
       "      <th>M_1</th>\n",
       "      <th>res_1</th>\n",
       "      <th>x&lt;2</th>\n",
       "      <th>M_2</th>\n",
       "      <th>res_2</th>\n",
       "      <th>M_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i_1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_4</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_5</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_6</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x  y  M_0  res_0    x>3  M_1  res_1    x<2   M_2  res_2    M_3\n",
       "i_1  0  1  2.0   -1.0  False -0.5   -0.5   True -0.50   0.00  0.125\n",
       "i_2  1  1  2.0   -1.0  False -0.5   -0.5   True -0.50   0.00  0.125\n",
       "i_3  2  2  2.0    0.0  False -0.5    0.5  False  0.25   0.25  0.125\n",
       "i_4  3  2  2.0    0.0  False -0.5    0.5  False  0.25   0.25  0.125\n",
       "i_5  4  3  2.0    1.0   True  1.0    0.0  False  0.25  -0.25 -0.250\n",
       "i_6  5  3  2.0    1.0   True  1.0    0.0  False  0.25  -0.25 -0.250"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GBM\n",
    "df = pd.DataFrame({\"x\":range(6),\"y\":[1,1,2,2,3,3]},index=[\"i_\"+str(i) for i in range(1,7)])\n",
    "df[\"M_0\"] = df[\"y\"].mean()\n",
    "df[\"res_0\"] = df[\"y\"]-df[\"M_0\"]\n",
    "df[\"x>3\"]=df[\"x\"]>3\n",
    "group_means = df.groupby(\"x>3\")[\"res_0\"].mean()  # prediction of M_1\n",
    "df[\"M_1\"] = [group_means[g] for g in df[\"x>3\"]]\n",
    "df[\"res_1\"] = df[\"res_0\"] - df[\"M_1\"]\n",
    "df[\"x<2\"] = df[\"x\"]<2\n",
    "group_means = df.groupby(\"x<2\")[\"res_1\"].mean()\n",
    "df[\"M_2\"] = [group_means[g] for g in df[\"x<2\"]]\n",
    "df[\"res_2\"]= df[\"res_1\"]-df[\"M_2\"]\n",
    "group_means = df.groupby(\"x>3\")[\"res_2\"].mean()\n",
    "df[\"M_3\"] = [group_means[g] for g in df[\"x>3\"]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b\n",
    "def gbm(df_orig, depth, no_trees):\n",
    "    df = df_orig.copy()\n",
    "    regression_values = df[\"REGRESSION\"]\n",
    "    # M0, which is the average regression value in the df\n",
    "    models = [regression_values.mean()]\n",
    "    predictions = np.array([models[0] for r in regression_values])\n",
    "    for i in range(1, no_trees+1):\n",
    "        # the original regression value for the instance minus\n",
    "        # the sum of the predictions of the previous models\n",
    "        # (M0,...Mi-1) for the instance\n",
    "        target_values = regression_values - predictions\n",
    "        df[\"REGRESSION\"] = target_values\n",
    "        reg_tree = regression_tree(df, depth)\n",
    "        models.append(reg_tree)\n",
    "        predictions += predict(df, reg_tree)\n",
    "    return models\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020-01-09\n",
    "\n",
    "### 1e Clustering\n",
    "Agglomerative clustering is deterministic, unless we handle encountered ties\n",
    "(regarding what clusters to merge) randomly. If we have chosen to employ single-\n",
    "linkage, will such a random selection of a pair of clusters to merge (from the\n",
    "alternative mergings that result in the same score) have any eﬀect on subsequent\n",
    "mergings that result in higher scores? Explain your reasoning.\n",
    "\n",
    "\n",
    "### Solution\n",
    "Ties appear when performing agglomerative (bottom-up) clustering when two or more alternatives of clusters to merge result in the same score. In general, resolving such ties randomly, i.e., picking any of the alternatives arbitrarily with some element of chance, could result in completely different clusterings, and would hence motivate multiple re-runs, similar to what is recommended for k-means clustering. (IF no such ties, it becomes completely deterministic and is not needed to re-run). When single-linage is used, the score is the smallest distance between a pair of elements in two different clusters. This means that in case of ties between several alternative clusters to merge, these will be merged in sequence. It is impossible to have clusters with smaller distance than the tied clusters. So the exact order in which this is done has no effect on sub-sequent mergings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Python': 0, 'Julia': 1, 'R': 2},\n",
       " array([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [0., 0., 1.]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2a\n",
    "# input: lists of words\n",
    "# output: (mapping, matrix)\n",
    "def bag_of_words(word_lists):\n",
    "    mapping = {}\n",
    "    index = 0\n",
    "    for p in word_lists:\n",
    "        for w in p:\n",
    "            if w not in mapping.keys():\n",
    "                mapping[w] = index\n",
    "                index += 1\n",
    "    M = len(word_lists)\n",
    "    N = len(mapping)\n",
    "    matrix = np.zeros((M,N))\n",
    "    for i,p in enumerate(word_lists):\n",
    "        for w in p:\n",
    "            matrix[i][mapping[w]] += 1\n",
    "    return (mapping, matrix)\n",
    "\n",
    "bag_of_words([[\"Python\"],[\"Python\",\"Julia\"],[\"R\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b\n",
    "def variable_importance(df, model):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020-04-14\n",
    "\n",
    "### 1b\n",
    "Assume that we want to discretize numerical features prior to applying the\n",
    "decision-tree learning algorithm. Will it have any eﬀect on the resulting tree if\n",
    "we employ equal-width or equal-sized binning? Explain your reasoning.\n",
    "\n",
    "### Solution\n",
    "If equal-width binning is employed, it may result in that the training instances are not distributed uniformly among the child nodes, when splitting a node during tree growth with the discretized feature. In the extreme case, this could mean that few instances are separated out, reducing the likelihood of the feature being selected, and increasing tree depth, if the feature is indeed selected. If equal-sized binning is employed, the instances will be distributed uniformly over the child nodes, typically leading to shallower trees, if the feature is selected.\n",
    "\n",
    "### 1d\n",
    "The predictive performance of an ensemble of classiﬁers, for which the predic-\n",
    "tions are formed by averaging predictions of the individual members, is depen-\n",
    "dent on the diversity of the members. Describe how an ensemble of naive Bayes\n",
    "classiﬁers would be trained, if similar techniques that are used to form random\n",
    "forests would be employed.\n",
    "\n",
    "\n",
    "### Solution\n",
    "If we would like to generate an ensemble of naive Bayes classifiers using the strategy of random forest, it would mean that we introduce diversity in two ways: by training each classifier from a bootstrap replicate (through bagging) and by considering a random subset of the features for each classifier.\n",
    "\n",
    "### 1e\n",
    "Assume that we have generated a set of association rules with a speciﬁed support\n",
    "and conﬁdence, from a dataset with a set of binary features and binary class\n",
    "labels, encoded as itemsets. Assume that we have selected a subset of the rules,\n",
    "for which the heads (consequents) contain only a class label. If we want to use\n",
    "this subset of rules to classify a novel test instance, i.e., to assign one of the two\n",
    "class labels, what are the potential problems we may encounter? Explain your\n",
    "reasoning\n",
    "\n",
    "### Solution\n",
    "It may be that multiple rules are applicable, i.e., the conditions (antecedents) are subsets of the itemset representing the instance to be classifiers, but with different consequents; this means that the rules are in conflict regarding what class label to assign.\n",
    "\n",
    "It may be that for some test instance, there is no rule such that the condition is a subset of the itemset representing the instance to be classified; this means that there is no applicable rule that can suggest what class label to assign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID   V1   V2 CLASS\n",
       "0  1.0  1.5  0.5     A\n",
       "1  2.0  3.5  0.5     B\n",
       "2  3.0  5.0  1.0     C"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aggregate(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    new_df_indexes = df[\"ID\"].unique()\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[\"ID\"] = new_df_indexes\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col != \"CLASS\":\n",
    "            ##################### remember #############\n",
    "            new_df[col] = [df.loc[df[\"ID\"]==i, col].mean() for i in new_df_indexes]\n",
    "        else:\n",
    "            new_df[col] = [df.loc[df[\"ID\"]==i, col].mode()[0] for i in new_df_indexes]\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "df2 = aggregate(pd.DataFrame({\"ID\":[1,1,2,2,3],\"V1\":[1,2,3,4,5],\"V2\":[1,0,1,0,1],\"CLASS\":[\"A\",\"A\",\"B\",\"C\",\"C\"]}))\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    V1   V2\n",
       "1  3.5  0.5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.loc[df2[\"ID\"]==2.0,[\"V1\",\"V2\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b\n",
    "def stacking(level0_df, level1_df, base_learners, learner):\n",
    "    base_models = [alg.fit(level0_df) for alg in base_learners]\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[\"CLASS\"] = level1_df[\"CLASS\"]\n",
    "    for i, model in enumerate(base_models):\n",
    "        new_df[i] = model.predict(level1_df)\n",
    "    stacking_model = learner.fit(new_df)\n",
    "    return base_models, stacking_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
