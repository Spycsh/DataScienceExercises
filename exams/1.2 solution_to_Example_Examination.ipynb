{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to Exam: Programming for Data Science (ID2214)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hereby declare that I have developed the solutions below without communicating with any other individual or forum, and without searching for any part of the solutions online. I also declare that I have not contributed to the solutions submitted by any other student on the exam. \n",
    "\n",
    "Date: 2020-10-02\n",
    "Name: Henrik Boström\n",
    "Personal no. : xxxxxx-xxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated accuracy of a model will vary depending on what set of instances\n",
    "is used to evaluate its performance. If we consider \n",
    "such a set of instances to be a random sample drawn from some\n",
    "(unknown) target distribution, the estimated accuracy of the model will sometimes be lower and\n",
    "sometimes higher than the true accuracy (the accuracy of the model wrt. the target distribution). \n",
    "If the compared models have similar true accuracies, then the observed differences in the estimated\n",
    "accuracies will mainly be due to the sample we have drawn, and hence the extreme values of the observed\n",
    "estimated accuracies will be biased; the lowest value will be overly pessimistic and the highest value overly optimistic.\n",
    "\n",
    "Hence, if we select the highest of these values as an estimate for the true accuracy of the best model,\n",
    "we will systematically be overestimating the performance. However, if one of the models is much more accurate than the others (and hence would almost always be outperforming the others independently of the sample used), then this bias will be smaller. Since we typically do not beforehand know what the true accuracies are, there is hence a high risk that the estimated accuracy is indeed too high. It should be noted though that this effect to some extent is compensated by the fact that 10-fold cross-validation is employed, which provides an estimate for the model performance when having trained on 9/10 of the data, which can be expected to be lower than when using a model trained on the entire sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated standard deviation is used as a denominator when z-normalization is applied,\n",
    "and since this will be reduced due to that missing values will be replaced by the mean,\n",
    "it means that values that are different from the mean will end up more far away from the mean\n",
    "after missing values have been imputed. As the minimum and maximum values are not affected by mean-value\n",
    "imputation, min-max normalization will result in the same values as before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking for the class label $c$ that maximizes $P(c|f_1=v_1, \\ldots, f_n=v_n)$ for the features $f_1, \\ldots, f_n$ and values $v_1, \\ldots, v_n$, using naïve Bayes, the class prior $P(c)$ is multiplied with $\\prod_{i=1}^{n}P(f_i=v_i|c)$.\n",
    "\n",
    "Hence, all features for which $P(f=0|c_1) = P(f=0|c_2)$ (and hence $P(f=1|c_1) = P(f=1|c_2)$, since the features are binary and the probabilities sum to one) will have no effect on the predicted class probabilities, and hence the accuracy will not be affected.\n",
    "\n",
    "However, for kNN the added features will be taken into account in the distance calculations, possibly changing the neighborhood of the test instances, and hence also result in different predictions, which may have a (positive or negative) effect on the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a binary classification task with the class labels + and -, and that we have three positive (+) instances p1, p2, p3 and two negative (-) instances n1, n2.\n",
    "\n",
    "If M1 predicts the following probabilities for the class +:\n",
    "\n",
    "n1: 0.9, p1: 0.8, p2: 0.7, p3: 0.6, n2: 0.1\n",
    "\n",
    "then the accuracy is 4/5 = 0.8 (assuming that the label with the highest probability is predicted), while the AUC is 1/2*(0/3)+1/2*(3/3) = 0.5\n",
    "\n",
    "If M2 predicts the following probabilities for the class +:\n",
    "\n",
    "p1: 0.6, p2: 0.4, n1: 0.3, p3: 0.2, n2: 0.1\n",
    "\n",
    "Then the accuracy is 3/5 = 0.6, while the AUC is 1/2*(2/3)+1/2*(3/3) = 0.833"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating a random forest, only a random subset of the features are evaluated\n",
    "when searching for the best way to split a node in a tree. If this subset constitutes\n",
    "less than one percent of the original features, then the cost of growing\n",
    "hundred trees is expected to be less than growing a single tree while evaluating  all features\n",
    "(as the computational cost is directly proportional to the number of evaluated features).\n",
    "Moreover, as the trees in the random forests are generated from bootstrap replicates, \n",
    "the full-grown trees can be expected to be shallower, and hence faster to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def create_one_hot(dataframe, min_freq=5):\n",
    "    df = dataframe.copy()\n",
    "    # from column name to a set of categorical values\n",
    "    one_hot_dict = {}\n",
    "    for col in df:\n",
    "        if (df.dtypes[col] == \"object\" and col !=\"CLASS\"):\n",
    "            counts = df[col].value_counts()\n",
    "            values = [v for v in counts.index if counts[v]>=min_freq]\n",
    "            one_hot_dict[col] = values\n",
    "            new_cols = [(str(col)+\"-\"+value, value) for value in values]\n",
    "            for new_col, value in new_cols:\n",
    "                df[new_col] = (df[col]==value).astype(\"float\")\n",
    "            df.drop(col,axis=\"columns\",inplace=True)\n",
    "    return df, one_hot_dict\n",
    "            \n",
    "\n",
    "dataframe = pd.DataFrame({\"color\":[\"red\",\"blue\",\"red\",\"green\",\"green\",\"green\"]})\n",
    "df,_=create_one_hot(dataframe, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color-green</th>\n",
       "      <th>color-red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color-green  color-red\n",
       "0          0.0        1.0\n",
       "1          0.0        0.0\n",
       "2          0.0        1.0\n",
       "3          1.0        0.0\n",
       "4          1.0        0.0\n",
       "5          1.0        0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(orig_df,min_freq=5):\n",
    "        df = orig_df.copy()\n",
    "        one_hot = {}\n",
    "        for col in df.columns:\n",
    "            if (df.dtypes[col] == \"object\") and (col != \"CLASS\"):\n",
    "                counts = df[col].value_counts()\n",
    "                values = [v for v in counts.index if counts[v] >= min_freq]\n",
    "                one_hot[col] = values\n",
    "                new_cols_vals = [(col+\"-\"+str(val),val) for val in values]\n",
    "                for (new_col,val) in new_cols_vals:\n",
    "                        df[new_col] = (df[col] == val).astype(\"float\")\n",
    "                df.drop(col,axis=\"columns\",inplace=True)\n",
    "        return df, one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is an already-built tree\n",
    "# N (node) = [V]  leaf, V is the predicted value (float)\n",
    "# N = [feature, threshold, left_child_node, right_child_node, left_weight] not leaf, but a child node\n",
    "# instance: a row of pandas data series\n",
    "def predict(T, i):\n",
    "    return dfs(T, 0, i)\n",
    "\n",
    "def dfs(tree, node_no, instance):\n",
    "    node = tree[node_no]\n",
    "    if len(node) == 1:\n",
    "        prediction = node[0]\n",
    "    else:\n",
    "        feature, threshold, left_child_node, right_child_node, left_weight = node\n",
    "        value = instance[feature]\n",
    "        if np.isnan(value):   # no feature\n",
    "            left_prediction = dfs(tree, left_child_node, instance)\n",
    "            right_prediction = dfs(tree, right_child_node, instance)\n",
    "            prediction = left_weight * left_prediction + (1-left_weigth)*right_prediction\n",
    "        elif value<=threshold:  # feature in the left sub-tree\n",
    "            prediction = dfs(tree, left_child_node, instance)\n",
    "        else:\n",
    "            prediction = dfs(tree, right_child_node, instance)\n",
    "    return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(T,i):\n",
    "    return make_prediction(T,0,i)\n",
    "\n",
    "def make_prediction(tree,node_no,instance):\n",
    "    node = tree[node_no]\n",
    "    if len(node) == 1:\n",
    "        prediction = node[0]\n",
    "    else:\n",
    "        feature,threshold,left_child_node,right_child_node,left_weight = node\n",
    "        value = instance[feature]\n",
    "        if np.isnan(value):\n",
    "            left_prediction = make_prediction(tree,left_child_node,instance)\n",
    "            right_prediction = make_prediction(tree,right_child_node,instance)\n",
    "            prediction = left_weight*left_prediction+(1-left_weight)*right_prediction\n",
    "        elif value <= threshold:\n",
    "            prediction = make_prediction(tree,left_child_node,instance)\n",
    "        else:\n",
    "            prediction = make_prediction(tree,right_child_node,instance)\n",
    "    return prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
